---
title: "ARIMA ANALISIS"
output:
  html_document: default
date: "2024-04-10"
pdf_document:
    fig_align: "center"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

Cargamos las librerias necesarias
```{r}
library(readxl)
library(forecast)   
library(dplyr)
library(tseries)
library(lmtest)

```

Leemos nuestro dataset y configuramos el numero de dias que usaremos para test.
```{r}
data <- read_excel("C:/Users/nicov/Desktop/TFG_code_local/src/datasets/dataset_input_v1.xlsx")
data[data < 0] <- 0

validation_days= 15
test_days = 7
```

Puesto que disponemos de un historico grande con datos muy alejados de los valores actuales decidimos entrenar el modelo con datos a partir de este año 2024.
```{r}
# Filtrar los datos a partir de 2024
data_filtrada <- data %>%
  filter(fechaHora >= as.POSIXct("2024-01-01"))

data_filtrada <- data_filtrada[complete.cases(data_filtrada), ]

```

Dividimos el dataset en train, validation y test. Debemos crear tambien conjuntos de validation y test de variables exogenas con las mismas trasnformaciones que se le aplicaran a la serie temporal para evaluar coherentemente el modelo SARIMA. 
```{r}

# Calcular la fecha de inicio del conjunto de validación
start_val <- as.POSIXct(as.Date(max(data_filtrada$fechaHora)) - (validation_days + test_days))

# Calcular la fecha de inicio del conjunto de prueba
start_test <- as.POSIXct(as.Date(max(data_filtrada$fechaHora)) - test_days)

# Dividir el DataFrame en conjunto de entrenamiento, validación y prueba

train <- data_filtrada[data_filtrada$fechaHora < start_val, ]
val <- data_filtrada[(start_val <= data_filtrada$fechaHora) & (data_filtrada$fechaHora < start_test), ]
test <- data_filtrada[start_test <= data_filtrada$fechaHora, ]


exog_train_raw <-  subset(train, select = -c(precio_spot, fechaHora))
exog_train <- lapply(exog_train_raw, function(x) diff(x))
exog_train <- lapply(exog_train, function(x) diff(x, lag = 24))
exog_train <- do.call(cbind, exog_train)

exog_val_raw <-  subset(val, select = -c(precio_spot, fechaHora))
exog_val <- lapply(exog_val_raw, function(x) diff(x))
exog_val <- lapply(exog_val, function(x) diff(x, lag = 24))
exog_val <- do.call(cbind, exog_val)

exog_test_raw <-  subset(test, select = -c(precio_spot, fechaHora))
exog_test <- lapply(exog_test_raw, function(x) diff(x))
exog_test <- lapply(exog_test, function(x) diff(x, lag = 24))
exog_test <- do.call(cbind, exog_test)


train_raw <- ts(train$precio_spot, frequency = 24, start = c(2024, 1, 1))
val_raw <- ts(val$precio_spot, frequency = 24)
test_raw <- ts(test$precio_spot, frequency = 24)



```

Como describimos en el apartado de Fundamentos Teoricos el modelo ARIMA tiene como supuesto que nuestra serie temporal es estacionaria en media en varianza, i.e, que la media y la varianza son estables en el tiempo.

Por ello visualizaremos la serie temporal, descomponiendola y realizaremos los contrastes de hipotesis oportunos para determinar si nos encontramos ante una serie temporal estacionaria y si se presenta una componente estacional. 
```{r}

componentes.ts = decompose(train_raw)
plot(componentes.ts)
```
Al descomponer la serie temporal observamos que esta no es ni estacionaria en media ni en varianza. Por ello deberemos realizar las transformaciones adecuadas para disponer de una serie temporal estacionaria adecuada para el modelo ARIMA. 

Parece que si existe una componente estacional, pero es dificil de determinar a partir de este grafico  por lo agrupados que estan los datos. Evaluaremos la existencia de componente estacional posteriormente mediante las ACF y PACF.

EL modelo ARIMA tiene como supuesto tratar con una serie estacionaria en media y en varianza. Puesto que nuestra serie temporal no lo es, debemos realizar las transformaciones necesarias.

Para hacer la serie temporal estacionaria en varianza realizaremos una transformacion de Box-Cox. En primer lugar calculamos el valor de lambda.
```{r}
lambda <- BoxCox.lambda(train_raw)
lambda
```

```{r}

Bc_tsData <- BoxCox(train_raw, lambda)
plot(cbind(train_raw,Bc_tsData))

```
Observamos como ahora la serie es estacionaria en varianza. Ahora deberemos hacerla estacionaria en media, por lo que diferenciamos las veces necesarias la serie. En nuestro caso tan solo sera necesario diferenciar una vez. El parametro 'd' de nuesto modelo SARIMA sera igual a 1.


```{r}
ndiffs(Bc_tsData)
train_stationary <- diff(Bc_tsData)
ndiffs(train_stationary)
```
```{r}
plot(train_stationary)
```
Como podemos observar la serie ahora es estacionaria en media y varianza.

Procedemos ahora a estudiar la estacionalidad.

```{r}
par(mfrow = c(1,2))
acf(train_stationary,lag.max=48)
pacf(train_stationary,lag.max=48)
```
Como podemos ver en la PACF la serie temporal tiene el valor mas significativo en el retardo (lag) 24 (valor de la frecuencia), por lo tanto la serie tiene componente estacional.

Realizamos una diferenciacion estacional. El parametro 'D' de ARIMA sera igual a 1.
```{r}
train_stationary <- diff(train_stationary, lag=24)
```

```{r}
#componentes.ts = decompose(train_stationary)
plot(train_stationary)
```

En principio ya hemos transformado nuestra serie a una estacionaria en media y varianza y sin componente estacional. Aun asi, realizamos el test de Dickie-Fuller aumentado para estar seguros de que no debemos realizar mas trasnformaciones. La hipotesis nula es que nuestra serie no es estacionaria.

```{r}
adf.test(train_stationary, alternative = "stationary")

```
Como el p-valor es menor que 0.05 podemos rechazar la hipotesis nula de que la serie no es estacionaria, por tanto podemos concluir que nuestra serie es estacionaria. 

Una vez tenemos nuestra serie temporal estacionaria podemos comenzar con la modelizacion. En primer lugar  graficaremos las ACF y PACF para hallar los parametros del modelo ARIMA(p,d,q)x(P,D,Q)s. 
```{r}
par(mfrow = c(1,2))
acf(train_stationary,lag.max=24)
pacf(train_stationary,lag.max=24)

```
Observamos en estas graficas como no hay ningun termino significativo ni autorregresivo ni de medias móviles consecutivo, por lo que los valores p y q seran igual a 0. Los parametros 'd' y 'D' tendran valor 1 por las diferenciaciones realizadas a la serie temporal y 's' sera 24 por su componente estacional.

Tras haber realizado este estudio estadistico podemos concluir que nuestro modelo SARIMA sera de la forma (0,1,0)x(0,1,0)24

Una vez disponemos de los parametros de nuestro modelo SARIMA creamos el modelo. Antes de entrenarlo deberemos ajustar las variables exogenas diferenciandolas para que tengan igual longuitud que la serie temporal transformada.

```{r}
arima <- Arima(train_stationary, order = c(0,1,0), seasonal = list(order = c(0,1,0), period = 24), xreg=exog_train)
summary(arima)

```
A priori no parece que tengamos un mal modelo, en general las metricas tienen valores bajos y un AIC=11631.238578.26
```{r}
coeftest(arima)
```
Una vez entrenado el modelo SARIMA debemos diagnosticar los residuos verificando que cumple los supuesto ARIMA:

-Autocorrelación: Los residuos no deben mostrar autocorrelación significativa, es decir, no deben exhibir patrones discernibles en sus autocorrelogramas

-Normalidad: Los residuos del modelo deben seguir una distribución normal.

-Estacionariedad: Los residuos deben ser estacionarios, lo que significa que su media y varianza deben ser constantes a lo largo del tiempo

Verificamos la autocorrelacion.
```{r}
par(mar = c(5, 4, 2, 1))  # Ajustar los márgenes inferior, izquierdo, superior y derecho

par(mfcol= c(2,1))
acf(arima$residuals, lag.max=107, main="ACF de los residuos estandarizados")
pacf(arima$residuals, lag.max=107, main="PACF de los residuos estandarizados")

```
La ACF y la PACF de los residuos  no parecen mostrar estructura y tienen casi todos los valores
dentro de las bandas de confianza. Aun asi en la PACF parece que hay un retardo constante que se sale de las bandas de confianza. No podemos garantizar que los residuos son aleatorios, pero tampoco lo contrario.

Realizamos el test de Ljung-Box.
```{r}

checkresiduals(arima)
```
El p-valor de text de Ljung-Box es menor que 0.05 luego se puede rechazar que las primeras autocorrelaciones
sean nulas, y no se puede asumir que los residuos sean ruido blanco. En la ACF podemos ver una notable correlacion con lag igual a 24. Los residuos sí parecen ajustarse a una distribucion normal. En la grafica de los residuos podemos observar que si parecen ser estacionarios.


Visualizamos el QQ Plot de los residuos.
```{r}
qqnorm(arima$residuals, ylab = "Residuals")
qqline(arima$residuals)


```
Podemos ver que los residuos aproximadamente siguen una distribución normal, a pesar de la gran cantidad
de valores atípicos.
```{r}
boxplot(arima$residuals,main="Boxplot de los residuos ")
```
En este box plot observamos mas facilmente la cantidad de valores atipicos presentes. Aun asi la mayoria de residuos estan entorno al 0.

Tras analizar los residuos todo apunta a que no siguen los supuestos ARIMA. Aun asi, si el modelo tiene buena capacidad predictiva no lo descartaremos.

Veamos gráficamente la diferencia entre la serie original y el modelo ajustado (en rojo)
```{r}
par(mfrow = c(1,1))
plot(tail(train_stationary, n = 72))
lines(tail(train_stationary, n = 72) - tail(arima$residuals, n = 72),col="red")


```
Como se puede apreciar el modelo no se esta ajustando muy bien a nuestra serie.

Realizamos predicciones con los datos de validacion para evaluar el modelo:
```{r}
predicciones <- forecast(arima, h = 24, xreg =as.matrix(exog_val), level = c(85, 95))

```

Debemos realizar las mismas trasnformaciones que hicimos al conjunto de train para obtener unas metricas adecuadas.
```{r}
Bc_val <- BoxCox(val$precio_spot, lambda)
lambda <- BoxCox.lambda(train_raw)
val_stationary <- diff(Bc_val)
val_stationary <- diff(val_stationary, lag=24)
```


```{r}

accuracy(predicciones, val_stationary)
```

Como podemos ver nuestro modelo no es muy bueno.Por ello haremos uso de la funcion autoarima para obtener el mejor modelo ARIMA desde un enfoque empirico en vez de teorico.


```{r}
#auto_arima <- auto.arima(train_stationary, xreg = exogData)

```
El mejor modelo hallado es con los parametros ARIMA(2,0,0)(2,0,0)[24].

Analogo al entrenamiento del modelo teorico.

```{r}
# Ajustar el modelo ARIMA
arima <- Arima(train_stationary, order = c(2,0,0), seasonal = list(order = c(2,1,0), period = 24), xreg=exog_train)
summary(arima)

```
En primera instancia se observa una ligera mejora en entrenamiento respecto al modelo anterior. EL AIC=8578.26 ha disminuido notablemente.
```{r}
coeftest(arima)
```
Volvemos a realizar una diagnsosis del modelo.
```{r}
par(mar = c(5, 4, 2, 1))  
par(mfcol= c(2,1))
acf(arima$residuals, lag.max=107, main="ACF de los residuos estandarizados")
pacf(arima$residuals, lag.max=107, main="PACF de los residuos estandarizados")

```
En esta ocasion si que parece que las funciones de autocorrelacion no muestran estructura y casi todos los valores se encuentran dentro de las bandas así que podemos considerar que los residuos son aleatorios

```{r}

checkresiduals(arima)
```
El p-valor de text de Ljung-Box es menor que 0.05 luego se puede rechazar que las primeras autocorrelaciones
sean nulas, y no se puede asumir que los residuos sean ruido blanco. En la ACF podemos ver una notable correlacion con lag igual a 24 y multiplos de él. Los residuos sí parecen ajustarse a una distribucion normal. En la grafica de los residuos podemos observar que si parecen ser estacionarios.


```{r}

# Crear el gráfico Q-Q de los residuos
qqnorm(arima$residuals, ylab = "Residuals")

# Agregar la línea de referencia al gráfico Q-Q
qqline(arima$residuals)


```
Podemos ver que los residuos aproximadamente siguen una distribución normal, a pesar de la gran cantidad
de valores atípicos.
```{r}
boxplot(arima$residuals,main="Boxplot de los residuos ")
```
En este box plot observamos mas facilmente la cantidad de valores atipicos presentes. Aun asi la mayoria de residuos estan entorno al 0.

Tras analizar los residuos todo apunta a que no siguen los supuestos ARIMA. Aun asi, si el modelo tiene buena capacidad predictiva no lo descartaremos.

Veamos gráficamente la diferencia entre la serie original y el modelo ajustado (en rojo)

```{r}
par(mfrow = c(1,1))
plot(tail(train_stationary, n = 72))
lines(tail(train_stationary, n = 72) - tail(arima$residuals, n = 72),col="red")


```
Nuevamente el modelo no parece ajustarse muy bien a los datos.

Realizamos predicciones con los datos de validacion para evaluar el modelo:

```{r}
predicciones <- forecast(arima, h = 24, xreg =as.matrix(exog_val), level = c(85, 95))

```

Como el modelo no has sufrido mas transformaciones (d, D igual a 0) seguimos evaluando con los datos de validacion trasformados anteriormente,
```{r}
accuracy(predicciones, val_stationary)
```
Observamos como efectivamente obtenemos unos resultados mejores que con el anterior modelo. Aun asi hemos visto que el modelo SARIMA no se ajusta muy bien a la serie temporal, por lo que tambien descartamos este modelo.

Parece que un modelo SARIMAX no es la mejor aproximacion a nuestro problema. Aun asi, a modo de ultimo intento, trataremos de entrenar un modelo con la serie original, sin sufrir ninguna transformacion alguna. Hacemos uso de autoarima.

```{r}
exogData <-  subset(train, select = -c(precio_spot, fechaHora))
#auto_arima <- auto.arima(tsData, xreg = as.matrix(exogData))

```

El mejor modelo hallado es ARIMA(1,1,4)(1,0,0)[24]

```{r}
arima <- Arima(train_raw, order = c(1,1,4), seasonal = list(order = c(1,0,0), period = 24), xreg=as.matrix(exog_train_raw))
summary(arima)


```
Las metricas han empeorado notablemente, el AIC=15131.88 ha aumentado a casi el doble respecto al anteiror modelo.
```{r}
coeftest(arima)
```

```{r}
par(mar = c(5, 4, 2, 1))  # Ajustar los márgenes inferior, izquierdo, superior y derecho

par(mfcol= c(2,1))
acf(arima$residuals, lag.max=107, main="ACF de los residuos estandarizados")
pacf(arima$residuals, lag.max=107, main="PACF de los residuos estandarizados")

```
Vemos como no muestran estructura y casi todos los valores se encuentran dentro de las bandas así que podemos considerar que los residuos son aleatorios.


```{r}

checkresiduals(arima)
```
El p-valor de text de Ljung-Box es menor que 0.05 luego se puede rechazar que las primeras autocorrelaciones
sean nulas, y no se puede asumir que los residuos sean ruido blanco. En la ACF podemos ver una notable correlacion con lag igual a 12 curiosamente. Los residuos sí parecen ajustarse a una distribucion normal aunque parece haber valores atipicos muy alejados de 0. En la grafica de los residuos podemos observar que si parecen ser estacionarios.


```{r}
# Crear el gráfico Q-Q de los residuos
qqnorm(arima$residuals, ylab = "Residuals")

# Agregar la línea de referencia al gráfico Q-Q
qqline(arima$residuals)


```
Al igual que en los anterirores modelos los resiudos aproximadamente se ajustan a una distribucion normal. Aun asi en este modelo hay residuos mucho mas alejados que en los anteriores.
```{r}
boxplot(arima$residuals,main="Boxplot de los residuos ")
```
Efectivamente corroboramos que hay bastantes valores atipicos muy alejados de 0.

Al igual que los anteriores modelos no podemos considerar que el modelo ha superado la diagnosis.

```{r}

par(mfrow = c(1,1))
plot(tail(train_raw, n = 72))
lines(tail(train_raw, n = 72) - tail(arima$residuals, n = 72),col="red")


```
En esta ocasion si que parece ajustarse mejor a nuestra serie temporal. Realizamos predicciones.

```{r}
predicciones <- forecast(arima, h = 24, xreg =as.matrix(exog_test_raw), level = c(85, 95))
predicciones$mean[predicciones$mean < 0] <- 0
```


```{r}
accuracy(predicciones, test$precio_spot)
```
Parece que las metricas obtenidas son las peores, sin embargo acabamos de ver que este modelo es el que mejor se ajusta a nuestra serie, por tanto lo seleccionamos como el mejor de los 3.


Procedemos a evaluar su comportamiento con datos de test

```{r}
plot(test$precio_spot, type = "l", ylim = c(0, max(test$precio_spot, predicciones$mean)), xlab = "Tiempo", ylab = "Valor", main = "Últimos valores y predicciones")

lines(as.numeric(predicciones$mean), col = "red")

legend("topright", legend = c("Últimos valores", "Predicciones"), col = c("black", "red"), lty = 1)
```
Como podemos ver las predicciones se ajustan bastante bien a los datos de test comparado con los otros modelos. Este ultimo modelo modelo  ARIMA(1,1,4)(1,0,0)[24] sera nuestro  ganador entre los modelos SARIMAX, aun asi pretendemos lograr unas mejores predicciones.